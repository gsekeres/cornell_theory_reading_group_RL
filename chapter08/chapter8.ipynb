{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1 Dyna Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.18s/it]\n",
      "100%|██████████| 20/20 [00:52<00:00,  2.61s/it]\n",
      "100%|██████████| 5/5 [01:47<00:00, 21.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0, Prioritized Sweeping, maze size 54\n",
      "run 0, Prioritized Sweeping, maze size 216\n",
      "run 0, Prioritized Sweeping, maze size 486\n",
      "run 0, Prioritized Sweeping, maze size 864\n",
      "run 0, Prioritized Sweeping, maze size 1350\n",
      "run 0, Dyna-Q, maze size 54\n",
      "run 0, Dyna-Q, maze size 216\n",
      "run 0, Dyna-Q, maze size 486\n",
      "run 0, Dyna-Q, maze size 864\n",
      "run 0, Dyna-Q, maze size 1350\n",
      "run 1, Prioritized Sweeping, maze size 54\n",
      "run 1, Prioritized Sweeping, maze size 216\n",
      "run 1, Prioritized Sweeping, maze size 486\n",
      "run 1, Prioritized Sweeping, maze size 864\n",
      "run 1, Prioritized Sweeping, maze size 1350\n",
      "run 1, Dyna-Q, maze size 54\n",
      "run 1, Dyna-Q, maze size 216\n",
      "run 1, Dyna-Q, maze size 486\n",
      "run 1, Dyna-Q, maze size 864\n",
      "run 1, Dyna-Q, maze size 1350\n",
      "run 2, Prioritized Sweeping, maze size 54\n",
      "run 2, Prioritized Sweeping, maze size 216\n",
      "run 2, Prioritized Sweeping, maze size 486\n",
      "run 2, Prioritized Sweeping, maze size 864\n",
      "run 2, Prioritized Sweeping, maze size 1350\n",
      "run 2, Dyna-Q, maze size 54\n",
      "run 2, Dyna-Q, maze size 216\n",
      "run 2, Dyna-Q, maze size 486\n",
      "run 2, Dyna-Q, maze size 864\n",
      "run 2, Dyna-Q, maze size 1350\n",
      "run 3, Prioritized Sweeping, maze size 54\n",
      "run 3, Prioritized Sweeping, maze size 216\n",
      "run 3, Prioritized Sweeping, maze size 486\n",
      "run 3, Prioritized Sweeping, maze size 864\n",
      "run 3, Prioritized Sweeping, maze size 1350\n",
      "run 3, Dyna-Q, maze size 54\n",
      "run 3, Dyna-Q, maze size 216\n",
      "run 3, Dyna-Q, maze size 486\n",
      "run 3, Dyna-Q, maze size 864\n",
      "run 3, Dyna-Q, maze size 1350\n",
      "run 4, Prioritized Sweeping, maze size 54\n",
      "run 4, Prioritized Sweeping, maze size 216\n",
      "run 4, Prioritized Sweeping, maze size 486\n",
      "run 4, Prioritized Sweeping, maze size 864\n",
      "run 4, Prioritized Sweeping, maze size 1350\n",
      "run 4, Dyna-Q, maze size 54\n",
      "run 4, Dyna-Q, maze size 216\n",
      "run 4, Dyna-Q, maze size 486\n",
      "run 4, Dyna-Q, maze size 864\n",
      "run 4, Dyna-Q, maze size 1350\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.pq = []\n",
    "        self.entry_finder = {}\n",
    "        self.REMOVED = '<removed-task>'\n",
    "        self.counter = 0\n",
    "\n",
    "    def add_item(self, item, priority=0):\n",
    "        if item in self.entry_finder:\n",
    "            self.remove_item(item)\n",
    "        entry = [priority, self.counter, item]\n",
    "        self.counter += 1\n",
    "        self.entry_finder[item] = entry\n",
    "        heapq.heappush(self.pq, entry)\n",
    "\n",
    "    def remove_item(self, item):\n",
    "        entry = self.entry_finder.pop(item)\n",
    "        entry[-1] = self.REMOVED\n",
    "\n",
    "    def pop_item(self):\n",
    "        while self.pq:\n",
    "            priority, count, item = heapq.heappop(self.pq)\n",
    "            if item is not self.REMOVED:\n",
    "                del self.entry_finder[item]\n",
    "                return item, priority\n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "\n",
    "    def empty(self):\n",
    "        return not self.entry_finder\n",
    "\n",
    "# A wrapper class for a maze, containing all the information about the maze.\n",
    "# Basically it's initialized to DynaMaze by default, however it can be easily adapted\n",
    "# to other maze\n",
    "class Maze:\n",
    "    def __init__(self):\n",
    "        # maze width\n",
    "        self.WORLD_WIDTH = 9\n",
    "\n",
    "        # maze height\n",
    "        self.WORLD_HEIGHT = 6\n",
    "\n",
    "        # all possible actions\n",
    "        self.ACTION_UP = 0\n",
    "        self.ACTION_DOWN = 1\n",
    "        self.ACTION_LEFT = 2\n",
    "        self.ACTION_RIGHT = 3\n",
    "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
    "\n",
    "        # start state\n",
    "        self.START_STATE = [2, 0]\n",
    "\n",
    "        # goal state\n",
    "        self.GOAL_STATES = [[0, 8]]\n",
    "\n",
    "        # all obstacles\n",
    "        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
    "        self.old_obstacles = None\n",
    "        self.new_obstacles = None\n",
    "\n",
    "        # time to change obstacles\n",
    "        self.obstacle_switch_time = None\n",
    "\n",
    "        # initial state action pair values\n",
    "        # self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)))\n",
    "\n",
    "        # the size of q value\n",
    "        self.q_size = (self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions))\n",
    "\n",
    "        # max steps\n",
    "        self.max_steps = float('inf')\n",
    "\n",
    "        # track the resolution for this maze\n",
    "        self.resolution = 1\n",
    "\n",
    "    # extend a state to a higher resolution maze\n",
    "    # @state: state in lower resolution maze\n",
    "    # @factor: extension factor, one state will become factor^2 states after extension\n",
    "    def extend_state(self, state, factor):\n",
    "        new_state = [state[0] * factor, state[1] * factor]\n",
    "        new_states = []\n",
    "        for i in range(0, factor):\n",
    "            for j in range(0, factor):\n",
    "                new_states.append([new_state[0] + i, new_state[1] + j])\n",
    "        return new_states\n",
    "\n",
    "    # extend a state into higher resolution\n",
    "    # one state in original maze will become @factor^2 states in @return new maze\n",
    "    def extend_maze(self, factor):\n",
    "        new_maze = Maze()\n",
    "        new_maze.WORLD_WIDTH = self.WORLD_WIDTH * factor\n",
    "        new_maze.WORLD_HEIGHT = self.WORLD_HEIGHT * factor\n",
    "        new_maze.START_STATE = [self.START_STATE[0] * factor, self.START_STATE[1] * factor]\n",
    "        new_maze.GOAL_STATES = self.extend_state(self.GOAL_STATES[0], factor)\n",
    "        new_maze.obstacles = []\n",
    "        for state in self.obstacles:\n",
    "            new_maze.obstacles.extend(self.extend_state(state, factor))\n",
    "        new_maze.q_size = (new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions))\n",
    "        # new_maze.stateActionValues = np.zeros((new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions)))\n",
    "        new_maze.resolution = factor\n",
    "        return new_maze\n",
    "\n",
    "    # take @action in @state\n",
    "    # @return: [new state, reward]\n",
    "    def step(self, state, action):\n",
    "        x, y = state\n",
    "        if action == self.ACTION_UP:\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == self.ACTION_DOWN:\n",
    "            x = min(x + 1, self.WORLD_HEIGHT - 1)\n",
    "        elif action == self.ACTION_LEFT:\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == self.ACTION_RIGHT:\n",
    "            y = min(y + 1, self.WORLD_WIDTH - 1)\n",
    "        if [x, y] in self.obstacles:\n",
    "            x, y = state\n",
    "        if [x, y] in self.GOAL_STATES:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        return [x, y], reward\n",
    "\n",
    "# a wrapper class for parameters of dyna algorithms\n",
    "class DynaParams:\n",
    "    def __init__(self):\n",
    "        # discount\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        # probability for exploration\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # step size\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        # weight for elapsed time\n",
    "        self.time_weight = 0\n",
    "\n",
    "        # n-step planning\n",
    "        self.planning_steps = 5\n",
    "\n",
    "        # average over several independent runs\n",
    "        self.runs = 10\n",
    "\n",
    "        # algorithm names\n",
    "        self.methods = ['Dyna-Q', 'Dyna-Q+']\n",
    "\n",
    "        # threshold for priority queue\n",
    "        self.theta = 0\n",
    "\n",
    "\n",
    "# choose an action based on epsilon-greedy algorithm\n",
    "def choose_action(state, q_value, maze, dyna_params):\n",
    "    if np.random.binomial(1, dyna_params.epsilon) == 1:\n",
    "        return np.random.choice(maze.actions)\n",
    "    else:\n",
    "        values = q_value[state[0], state[1], :]\n",
    "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n",
    "\n",
    "# Trivial model for planning in Dyna-Q\n",
    "class TrivialModel:\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, rand=np.random):\n",
    "        self.model = dict()\n",
    "        self.rand = rand\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        if tuple(state) not in self.model.keys():\n",
    "            self.model[tuple(state)] = dict()\n",
    "        self.model[tuple(state)][action] = [list(next_state), reward]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        state_index = self.rand.choice(range(len(self.model.keys())))\n",
    "        state = list(self.model)[state_index]\n",
    "        action_index = self.rand.choice(range(len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[action_index]\n",
    "        next_state, reward = self.model[state][action]\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        return list(state), action, list(next_state), reward\n",
    "\n",
    "# Time-based model for planning in Dyna-Q+\n",
    "class TimeModel:\n",
    "    # @maze: the maze instance. Indeed it's not very reasonable to give access to maze to the model.\n",
    "    # @timeWeight: also called kappa, the weight for elapsed time in sampling reward, it need to be small\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, maze, time_weight=1e-4, rand=np.random):\n",
    "        self.rand = rand\n",
    "        self.model = dict()\n",
    "\n",
    "        # track the total time\n",
    "        self.time = 0\n",
    "\n",
    "        self.time_weight = time_weight\n",
    "        self.maze = maze\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        self.time += 1\n",
    "        if tuple(state) not in self.model.keys():\n",
    "            self.model[tuple(state)] = dict()\n",
    "\n",
    "            # Actions that had never been tried before from a state were allowed to be considered in the planning step\n",
    "            for action_ in self.maze.actions:\n",
    "                if action_ != action:\n",
    "                    # Such actions would lead back to the same state with a reward of zero\n",
    "                    # Notice that the minimum time stamp is 1 instead of 0\n",
    "                    self.model[tuple(state)][action_] = [list(state), 0, 1]\n",
    "\n",
    "        self.model[tuple(state)][action] = [list(next_state), reward, self.time]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        state_index = self.rand.choice(range(len(self.model.keys())))\n",
    "        state = list(self.model)[state_index]\n",
    "        action_index = self.rand.choice(range(len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[action_index]\n",
    "        next_state, reward, time = self.model[state][action]\n",
    "\n",
    "        # adjust reward with elapsed time since last vist\n",
    "        reward += self.time_weight * np.sqrt(self.time - time)\n",
    "\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "\n",
    "        return list(state), action, list(next_state), reward\n",
    "\n",
    "# Model containing a priority queue for Prioritized Sweeping\n",
    "class PriorityModel(TrivialModel):\n",
    "    def __init__(self, rand=np.random):\n",
    "        TrivialModel.__init__(self, rand)\n",
    "        # maintain a priority queue\n",
    "        self.priority_queue = PriorityQueue()\n",
    "        # track predecessors for every state\n",
    "        self.predecessors = dict()\n",
    "\n",
    "    # add a @state-@action pair into the priority queue with priority @priority\n",
    "    def insert(self, priority, state, action):\n",
    "        # note the priority queue is a minimum heap, so we use -priority\n",
    "        self.priority_queue.add_item((tuple(state), action), -priority)\n",
    "\n",
    "    # @return: whether the priority queue is empty\n",
    "    def empty(self):\n",
    "        return self.priority_queue.empty()\n",
    "\n",
    "    # get the first item in the priority queue\n",
    "    def sample(self):\n",
    "        (state, action), priority = self.priority_queue.pop_item()\n",
    "        next_state, reward = self.model[state][action]\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        return -priority, list(state), action, list(next_state), reward\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        TrivialModel.feed(self, state, action, next_state, reward)\n",
    "        if tuple(next_state) not in self.predecessors.keys():\n",
    "            self.predecessors[tuple(next_state)] = set()\n",
    "        self.predecessors[tuple(next_state)].add((tuple(state), action))\n",
    "\n",
    "    # get all seen predecessors of a state @state\n",
    "    def predecessor(self, state):\n",
    "        if tuple(state) not in self.predecessors.keys():\n",
    "            return []\n",
    "        predecessors = []\n",
    "        for state_pre, action_pre in list(self.predecessors[tuple(state)]):\n",
    "            predecessors.append([list(state_pre), action_pre, self.model[state_pre][action_pre][1]])\n",
    "        return predecessors\n",
    "\n",
    "\n",
    "# play for an episode for Dyna-Q algorithm\n",
    "# @q_value: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dyna_params: several params for the algorithm\n",
    "def dyna_q(q_value, model, maze, dyna_params):\n",
    "    state = maze.START_STATE\n",
    "    steps = 0\n",
    "    while state not in maze.GOAL_STATES:\n",
    "        # track the steps\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = choose_action(state, q_value, maze, dyna_params)\n",
    "\n",
    "        # take action\n",
    "        next_state, reward = maze.step(state, action)\n",
    "\n",
    "        # Q-Learning update\n",
    "        q_value[state[0], state[1], action] += \\\n",
    "            dyna_params.alpha * (reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n",
    "                                 q_value[state[0], state[1], action])\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(state, action, next_state, reward)\n",
    "\n",
    "        # sample experience from the model\n",
    "        for t in range(0, dyna_params.planning_steps):\n",
    "            state_, action_, next_state_, reward_ = model.sample()\n",
    "            q_value[state_[0], state_[1], action_] += \\\n",
    "                dyna_params.alpha * (reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) -\n",
    "                                     q_value[state_[0], state_[1], action_])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # check whether it has exceeded the step limit\n",
    "        if steps > maze.max_steps:\n",
    "            break\n",
    "\n",
    "    return steps\n",
    "\n",
    "# play for an episode for prioritized sweeping algorithm\n",
    "# @q_value: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dyna_params: several params for the algorithm\n",
    "# @return: # of backups during this episode\n",
    "def prioritized_sweeping(q_value, model, maze, dyna_params):\n",
    "    state = maze.START_STATE\n",
    "\n",
    "    # track the steps in this episode\n",
    "    steps = 0\n",
    "\n",
    "    # track the backups in planning phase\n",
    "    backups = 0\n",
    "\n",
    "    while state not in maze.GOAL_STATES:\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = choose_action(state, q_value, maze, dyna_params)\n",
    "\n",
    "        # take action\n",
    "        next_state, reward = maze.step(state, action)\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(state, action, next_state, reward)\n",
    "\n",
    "        # get the priority for current state action pair\n",
    "        priority = np.abs(reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n",
    "                          q_value[state[0], state[1], action])\n",
    "\n",
    "        if priority > dyna_params.theta:\n",
    "            model.insert(priority, state, action)\n",
    "\n",
    "        # start planning\n",
    "        planning_step = 0\n",
    "\n",
    "        # planning for several steps,\n",
    "        # although keep planning until the priority queue becomes empty will converge much faster\n",
    "        while planning_step < dyna_params.planning_steps and not model.empty():\n",
    "            # get a sample with highest priority from the model\n",
    "            priority, state_, action_, next_state_, reward_ = model.sample()\n",
    "\n",
    "            # update the state action value for the sample\n",
    "            delta = reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) - \\\n",
    "                    q_value[state_[0], state_[1], action_]\n",
    "            q_value[state_[0], state_[1], action_] += dyna_params.alpha * delta\n",
    "\n",
    "            # deal with all the predecessors of the sample state\n",
    "            for state_pre, action_pre, reward_pre in model.predecessor(state_):\n",
    "                priority = np.abs(reward_pre + dyna_params.gamma * np.max(q_value[state_[0], state_[1], :]) -\n",
    "                                  q_value[state_pre[0], state_pre[1], action_pre])\n",
    "                if priority > dyna_params.theta:\n",
    "                    model.insert(priority, state_pre, action_pre)\n",
    "            planning_step += 1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # update the # of backups\n",
    "        backups += planning_step + 1\n",
    "\n",
    "    return backups\n",
    "\n",
    "# Figure 8.2, DynaMaze, use 10 runs instead of 30 runs\n",
    "def figure_8_2():\n",
    "    # set up an instance for DynaMaze\n",
    "    dyna_maze = Maze()\n",
    "    dyna_params = DynaParams()\n",
    "\n",
    "    runs = 10\n",
    "    episodes = 50\n",
    "    planning_steps = [0, 5, 50]\n",
    "    steps = np.zeros((len(planning_steps), episodes))\n",
    "\n",
    "    for run in tqdm(range(runs)):\n",
    "        for i, planning_step in enumerate(planning_steps):\n",
    "            dyna_params.planning_steps = planning_step\n",
    "            q_value = np.zeros(dyna_maze.q_size)\n",
    "\n",
    "            # generate an instance of Dyna-Q model\n",
    "            model = TrivialModel()\n",
    "            for ep in range(episodes):\n",
    "                # print('run:', run, 'planning step:', planning_step, 'episode:', ep)\n",
    "                steps[i, ep] += dyna_q(q_value, model, dyna_maze, dyna_params)\n",
    "\n",
    "    # averaging over runs\n",
    "    steps /= runs\n",
    "\n",
    "    for i in range(len(planning_steps)):\n",
    "        plt.plot(steps[i, :], label='%d planning steps' % (planning_steps[i]))\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('steps per episode')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('./figure_8_2.png')\n",
    "    plt.close()\n",
    "\n",
    "# wrapper function for changing maze\n",
    "# @maze: a maze instance\n",
    "# @dynaParams: several parameters for dyna algorithms\n",
    "def changing_maze(maze, dyna_params):\n",
    "\n",
    "    # set up max steps\n",
    "    max_steps = maze.max_steps\n",
    "\n",
    "    # track the cumulative rewards\n",
    "    rewards = np.zeros((dyna_params.runs, 2, max_steps))\n",
    "\n",
    "    for run in tqdm(range(dyna_params.runs)):\n",
    "        # set up models\n",
    "        models = [TrivialModel(), TimeModel(maze, time_weight=dyna_params.time_weight)]\n",
    "\n",
    "        # initialize state action values\n",
    "        q_values = [np.zeros(maze.q_size), np.zeros(maze.q_size)]\n",
    "\n",
    "        for i in range(len(dyna_params.methods)):\n",
    "            # print('run:', run, dyna_params.methods[i])\n",
    "\n",
    "            # set old obstacles for the maze\n",
    "            maze.obstacles = maze.old_obstacles\n",
    "\n",
    "            steps = 0\n",
    "            last_steps = steps\n",
    "            while steps < max_steps:\n",
    "                # play for an episode\n",
    "                steps += dyna_q(q_values[i], models[i], maze, dyna_params)\n",
    "\n",
    "                # update cumulative rewards\n",
    "                rewards[run, i, last_steps: steps] = rewards[run, i, last_steps]\n",
    "                rewards[run, i, min(steps, max_steps - 1)] = rewards[run, i, last_steps] + 1\n",
    "                last_steps = steps\n",
    "\n",
    "                if steps > maze.obstacle_switch_time:\n",
    "                    # change the obstacles\n",
    "                    maze.obstacles = maze.new_obstacles\n",
    "\n",
    "    # averaging over runs\n",
    "    rewards = rewards.mean(axis=0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Figure 8.4, BlockingMaze\n",
    "def figure_8_4():\n",
    "    # set up a blocking maze instance\n",
    "    blocking_maze = Maze()\n",
    "    blocking_maze.START_STATE = [5, 3]\n",
    "    blocking_maze.GOAL_STATES = [[0, 8]]\n",
    "    blocking_maze.old_obstacles = [[3, i] for i in range(0, 8)]\n",
    "\n",
    "    # new obstalces will block the optimal path\n",
    "    blocking_maze.new_obstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # step limit\n",
    "    blocking_maze.max_steps = 3000\n",
    "\n",
    "    # obstacles will change after 1000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 1000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    blocking_maze.obstacle_switch_time = 1000\n",
    "\n",
    "    # set up parameters\n",
    "    dyna_params = DynaParams()\n",
    "    dyna_params.alpha = 1.0\n",
    "    dyna_params.planning_steps = 10\n",
    "    dyna_params.runs = 20\n",
    "\n",
    "    # kappa must be small, as the reward for getting the goal is only 1\n",
    "    dyna_params.time_weight = 1e-4\n",
    "\n",
    "    # play\n",
    "    rewards = changing_maze(blocking_maze, dyna_params)\n",
    "\n",
    "    for i in range(len(dyna_params.methods)):\n",
    "        plt.plot(rewards[i, :], label=dyna_params.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('./figure_8_4.png')\n",
    "    plt.close()\n",
    "\n",
    "# Figure 8.5, ShortcutMaze\n",
    "def figure_8_5():\n",
    "    # set up a shortcut maze instance\n",
    "    shortcut_maze = Maze()\n",
    "    shortcut_maze.START_STATE = [5, 3]\n",
    "    shortcut_maze.GOAL_STATES = [[0, 8]]\n",
    "    shortcut_maze.old_obstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # new obstacles will have a shorter path\n",
    "    shortcut_maze.new_obstacles = [[3, i] for i in range(1, 8)]\n",
    "\n",
    "    # step limit\n",
    "    shortcut_maze.max_steps = 6000\n",
    "\n",
    "    # obstacles will change after 3000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 3000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    shortcut_maze.obstacle_switch_time = 3000\n",
    "\n",
    "    # set up parameters\n",
    "    dyna_params = DynaParams()\n",
    "\n",
    "    # 50-step planning\n",
    "    dyna_params.planning_steps = 50\n",
    "    dyna_params.runs = 5\n",
    "    dyna_params.time_weight = 1e-3\n",
    "    dyna_params.alpha = 1.0\n",
    "\n",
    "    # play\n",
    "    rewards = changing_maze(shortcut_maze, dyna_params)\n",
    "\n",
    "    for i in range(len(dyna_params.methods)):\n",
    "        plt.plot( rewards[i, :], label=dyna_params.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('./figure_8_5.png')\n",
    "    plt.close()\n",
    "\n",
    "# Check whether state-action values are already optimal\n",
    "def check_path(q_values, maze):\n",
    "    # get the length of optimal path\n",
    "    # 14 is the length of optimal path of the original maze\n",
    "    # 1.2 means it's a relaxed optifmal path\n",
    "    max_steps = 14 * maze.resolution * 1.2\n",
    "    state = maze.START_STATE\n",
    "    steps = 0\n",
    "    while state not in maze.GOAL_STATES:\n",
    "        action = np.argmax(q_values[state[0], state[1], :])\n",
    "        state, _ = maze.step(state, action)\n",
    "        steps += 1\n",
    "        if steps > max_steps:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Example 8.4, mazes with different resolution\n",
    "def example_8_4():\n",
    "    # get the original 6 * 9 maze\n",
    "    original_maze = Maze()\n",
    "\n",
    "    # set up the parameters for each algorithm\n",
    "    params_dyna = DynaParams()\n",
    "    params_dyna.planning_steps = 5\n",
    "    params_dyna.alpha = 0.5\n",
    "    params_dyna.gamma = 0.95\n",
    "\n",
    "    params_prioritized = DynaParams()\n",
    "    params_prioritized.theta = 0.0001\n",
    "    params_prioritized.planning_steps = 5\n",
    "    params_prioritized.alpha = 0.5\n",
    "    params_prioritized.gamma = 0.95\n",
    "\n",
    "    params = [params_prioritized, params_dyna]\n",
    "\n",
    "    # set up models for planning\n",
    "    models = [PriorityModel, TrivialModel]\n",
    "    method_names = ['Prioritized Sweeping', 'Dyna-Q']\n",
    "\n",
    "    # due to limitation of my machine, I can only perform experiments for 5 mazes\n",
    "    # assuming the 1st maze has w * h states, then k-th maze has w * h * k * k states\n",
    "    num_of_mazes = 5\n",
    "\n",
    "    # build all the mazes\n",
    "    mazes = [original_maze.extend_maze(i) for i in range(1, num_of_mazes + 1)]\n",
    "    methods = [prioritized_sweeping, dyna_q]\n",
    "\n",
    "    # My machine cannot afford too many runs...\n",
    "    runs = 5\n",
    "\n",
    "    # track the # of backups\n",
    "    backups = np.zeros((runs, 2, num_of_mazes))\n",
    "\n",
    "    for run in range(0, runs):\n",
    "        for i in range(0, len(method_names)):\n",
    "            for mazeIndex, maze in zip(range(0, len(mazes)), mazes):\n",
    "                print('run %d, %s, maze size %d' % (run, method_names[i], maze.WORLD_HEIGHT * maze.WORLD_WIDTH))\n",
    "\n",
    "                # initialize the state action values\n",
    "                q_value = np.zeros(maze.q_size)\n",
    "\n",
    "                # track steps / backups for each episode\n",
    "                steps = []\n",
    "\n",
    "                # generate the model\n",
    "                model = models[i]()\n",
    "\n",
    "                # play for an episode\n",
    "                while True:\n",
    "                    steps.append(methods[i](q_value, model, maze, params[i]))\n",
    "\n",
    "                    # print best actions w.r.t. current state-action values\n",
    "                    # printActions(currentStateActionValues, maze)\n",
    "\n",
    "                    # check whether the (relaxed) optimal path is found\n",
    "                    if check_path(q_value, maze):\n",
    "                        break\n",
    "\n",
    "                # update the total steps / backups for this maze\n",
    "                backups[run, i, mazeIndex] = np.sum(steps)\n",
    "\n",
    "    backups = backups.mean(axis=0)\n",
    "\n",
    "    # Dyna-Q performs several backups per step\n",
    "    backups[1, :] *= params_dyna.planning_steps + 1\n",
    "\n",
    "    for i in range(0, len(method_names)):\n",
    "        plt.plot(np.arange(1, num_of_mazes + 1), backups[i, :], label=method_names[i])\n",
    "    plt.xlabel('maze resolution factor')\n",
    "    plt.ylabel('backups until optimal solution')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('./example_8_4.png')\n",
    "    plt.close()\n",
    "\n",
    "figure_8_2()\n",
    "figure_8_4()\n",
    "figure_8_5()\n",
    "example_8_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.2 Expected vs. Sample Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8850.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2621.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 220.16it/s]\n",
      "100%|██████████| 100/100 [00:13<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for figure 8.7, run a simulation of 2 * @b steps\n",
    "def b_steps(b):\n",
    "    # set the value of the next b states\n",
    "    # it is not clear how to set this\n",
    "    distribution = np.random.randn(b)\n",
    "\n",
    "    # true value of the current state\n",
    "    true_v = np.mean(distribution)\n",
    "\n",
    "    samples = []\n",
    "    errors = []\n",
    "\n",
    "    # sample 2b steps\n",
    "    for t in range(2 * b):\n",
    "        v = np.random.choice(distribution)\n",
    "        samples.append(v)\n",
    "        errors.append(np.abs(np.mean(samples) - true_v))\n",
    "\n",
    "    return errors\n",
    "\n",
    "def figure_8_7():\n",
    "    runs = 100\n",
    "    branch = [2, 10, 100, 1000]\n",
    "    for b in branch:\n",
    "        errors = np.zeros((runs, 2 * b))\n",
    "        for r in tqdm(np.arange(runs)):\n",
    "            errors[r] = b_steps(b)\n",
    "        errors = errors.mean(axis=0)\n",
    "        x_axis = (np.arange(len(errors)) + 1) / float(b)\n",
    "        plt.plot(x_axis, errors, label='b = %d' % (b))\n",
    "\n",
    "    plt.xlabel('number of computations')\n",
    "    plt.xticks([0, 1.0, 2.0], ['0', 'b', '2b'])\n",
    "    plt.ylabel('RMS error')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('./figure_8_7.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    figure_8_7()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.3 Trajectory Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:19<00:00, 1019.70it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.31it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1024.82it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1025.84it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1013.03it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1023.92it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1028.50it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.13it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1016.20it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.89it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1010.00it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1023.78it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1024.00it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.75it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.44it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.22it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.49it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.78it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.58it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.66it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.32it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1019.93it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.73it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.27it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.18it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.50it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1025.85it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.13it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1061.55it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1065.16it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1062.81it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1061.99it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1063.81it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.54it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.18it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.90it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1060.68it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.05it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1057.94it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.53it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.25it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.28it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.25it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.18it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.01it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.28it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1061.52it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.24it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.28it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.82it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.18it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1044.27it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.88it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.05it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1067.63it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1064.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1028.83it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1065.91it/s]\n",
      "100%|██████████| 20000/20000 [00:47<00:00, 424.02it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1039.94it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.09it/s]\n",
      "100%|██████████| 20000/20000 [10:19<00:00, 32.29it/s]  \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.84it/s]\n",
      "100%|██████████| 20000/20000 [01:10<00:00, 281.78it/s] \n",
      "100%|██████████| 20000/20000 [00:20<00:00, 981.75it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.96it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.22it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.77it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1005.54it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.22it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.71it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.10it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.06it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.29it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.11it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.65it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.66it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.12it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.91it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.03it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.64it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.61it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1025.73it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.87it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.82it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.01it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1057.49it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1063.74it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.78it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.25it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1052.03it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.01it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1063.92it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1060.64it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.48it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1062.79it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1057.45it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.34it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.01it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.92it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.24it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1066.88it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.62it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.60it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1056.55it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.05it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.32it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.00it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1065.42it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1052.00it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.39it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1062.37it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1052.02it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.70it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.11it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.12it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.81it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.50it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.80it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.86it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.68it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.30it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.94it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.87it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.95it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.24it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.31it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.25it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.80it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1028.84it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.10it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.84it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.97it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.44it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.97it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.91it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1039.08it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.27it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.34it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.63it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.79it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.04it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.30it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.95it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.25it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1052.71it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.97it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.71it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1056.32it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.88it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1061.33it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1060.95it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.44it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1060.42it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1056.45it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.86it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.19it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.59it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.14it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.07it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.26it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.36it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.54it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1058.27it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.59it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1056.81it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1055.97it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.71it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.93it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.31it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.52it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.30it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.72it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.40it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.70it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.68it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.10it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.01it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.86it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.48it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.64it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.99it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.99it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.09it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.73it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.50it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 954.52it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.72it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.28it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.54it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.23it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.73it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1024.09it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.89it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.98it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1044.13it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.93it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.35it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1057.72it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.79it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1056.43it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1004.78it/s]\n",
      "100%|██████████| 20000/20000 [00:21<00:00, 934.43it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1010.85it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.12it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.18it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.69it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.41it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.71it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.58it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1056.70it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1060.93it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.01it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.80it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1059.33it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.33it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.75it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.12it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.91it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1053.71it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.59it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1054.69it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.54it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.91it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1022.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1022.38it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.06it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1024.48it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.50it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1025.07it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.30it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1002.57it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 978.00it/s] \n",
      "100%|██████████| 20000/20000 [00:20<00:00, 996.01it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1010.50it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.18it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.42it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.37it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.49it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.69it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1020.68it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 988.95it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1019.47it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.97it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 959.31it/s] \n",
      "100%|██████████| 20000/20000 [00:21<00:00, 937.22it/s] \n",
      "100%|██████████| 20000/20000 [00:20<00:00, 972.97it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1004.70it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.33it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 988.88it/s] \n",
      "100%|██████████| 20000/20000 [00:20<00:00, 982.83it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.74it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.68it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.45it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.37it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1050.31it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.59it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.37it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.90it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.65it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.68it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.16it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.35it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.28it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.20it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.29it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.65it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.36it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.68it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.39it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.52it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.75it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.37it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.98it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.08it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.65it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.08it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.21it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.43it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.41it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.28it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.57it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.18it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1026.97it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.71it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1028.55it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.18it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1028.81it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.89it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.45it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.58it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1035.51it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1023.42it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1038.02it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.12it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.59it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1036.43it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1025.10it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.83it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1021.41it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.82it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1032.48it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.94it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.90it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1029.16it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1031.63it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.82it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1033.02it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 967.22it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.67it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.91it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.91it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.23it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.57it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.54it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.66it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.36it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1046.72it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.56it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1034.23it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.34it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.53it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1048.80it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1022.74it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 990.09it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1025.46it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1030.69it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.68it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1039.94it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1047.62it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1037.00it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1049.01it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 978.37it/s] \n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1027.00it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.30it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1040.69it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1043.83it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1042.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# 2 actions\n",
    "ACTIONS = [0, 1]\n",
    "\n",
    "# each transition has a probability to terminate with 0\n",
    "TERMINATION_PROB = 0.1\n",
    "\n",
    "# maximum expected updates\n",
    "MAX_STEPS = 20000\n",
    "\n",
    "# epsilon greedy for behavior policy\n",
    "EPSILON = 0.1\n",
    "\n",
    "\n",
    "# break tie randomly\n",
    "def argmax(value):\n",
    "    max_q = np.max(value)\n",
    "    return np.random.choice([a for a, q in enumerate(value) if q == max_q])\n",
    "\n",
    "\n",
    "class Task:\n",
    "    # @n_states: number of non-terminal states\n",
    "    # @b: branch\n",
    "    # Each episode starts with state 0, and state n_states is a terminal state\n",
    "    def __init__(self, n_states, b):\n",
    "        self.n_states = n_states\n",
    "        self.b = b\n",
    "\n",
    "        # transition matrix, each state-action pair leads to b possible states\n",
    "        self.transition = np.random.randint(n_states, size=(n_states, len(ACTIONS), b))\n",
    "\n",
    "        # it is not clear how to set the reward, I use a unit normal distribution here\n",
    "        # reward is determined by (s, a, s')\n",
    "        self.reward = np.random.randn(n_states, len(ACTIONS), b)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        if np.random.rand() < TERMINATION_PROB:\n",
    "            return self.n_states, 0\n",
    "        next_ = np.random.randint(self.b)\n",
    "        return self.transition[state, action, next_], self.reward[state, action, next_]\n",
    "\n",
    "\n",
    "# Evaluate the value of the start state for the greedy policy\n",
    "# derived from @q under the MDP @task\n",
    "def evaluate_pi(q, task):\n",
    "    # use Monte Carlo method to estimate the state value\n",
    "    runs = 1000\n",
    "    returns = []\n",
    "    for r in range(runs):\n",
    "        rewards = 0\n",
    "        state = 0\n",
    "        while state < task.n_states:\n",
    "            action = argmax(q[state])\n",
    "            state, r = task.step(state, action)\n",
    "            rewards += r\n",
    "        returns.append(rewards)\n",
    "    return np.mean(returns)\n",
    "\n",
    "\n",
    "# perform expected update from a uniform state-action distribution of the MDP @task\n",
    "# evaluate the learned q value every @eval_interval steps\n",
    "def uniform(task, eval_interval):\n",
    "    performance = []\n",
    "    q = np.zeros((task.n_states, 2))\n",
    "    for step in tqdm(range(MAX_STEPS)):\n",
    "        state = step // len(ACTIONS) % task.n_states\n",
    "        action = step % len(ACTIONS)\n",
    "\n",
    "        next_states = task.transition[state, action]\n",
    "        q[state, action] = (1 - TERMINATION_PROB) * np.mean(\n",
    "            task.reward[state, action] + np.max(q[next_states, :], axis=1))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            v_pi = evaluate_pi(q, task)\n",
    "            performance.append([step, v_pi])\n",
    "\n",
    "    return zip(*performance)\n",
    "\n",
    "\n",
    "# perform expected update from an on-policy distribution of the MDP @task\n",
    "# evaluate the learned q value every @eval_interval steps\n",
    "def on_policy(task, eval_interval):\n",
    "    performance = []\n",
    "    q = np.zeros((task.n_states, 2))\n",
    "    state = 0\n",
    "    for step in tqdm(range(MAX_STEPS)):\n",
    "        if np.random.rand() < EPSILON:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            action = argmax(q[state])\n",
    "\n",
    "        next_state, _ = task.step(state, action)\n",
    "\n",
    "        next_states = task.transition[state, action]\n",
    "        q[state, action] = (1 - TERMINATION_PROB) * np.mean(\n",
    "            task.reward[state, action] + np.max(q[next_states, :], axis=1))\n",
    "\n",
    "        if next_state == task.n_states:\n",
    "            next_state = 0\n",
    "        state = next_state\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            v_pi = evaluate_pi(q, task)\n",
    "            performance.append([step, v_pi])\n",
    "\n",
    "    return zip(*performance)\n",
    "\n",
    "\n",
    "def figure_8_8():\n",
    "    num_states = [1000, 10000]\n",
    "    branch = [1, 3, 10]\n",
    "    methods = [on_policy, uniform]\n",
    "\n",
    "    # average across 30 tasks\n",
    "    n_tasks = 30\n",
    "\n",
    "    # number of evaluation points\n",
    "    x_ticks = 100\n",
    "\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    for i, n in enumerate(num_states):\n",
    "        plt.subplot(2, 1, i+1)\n",
    "        for b in branch:\n",
    "            tasks = [Task(n, b) for _ in range(n_tasks)]\n",
    "            for method in methods:\n",
    "                steps = None\n",
    "                value = []\n",
    "                for task in tasks:\n",
    "                    steps, v = method(task, MAX_STEPS / x_ticks)\n",
    "                    value.append(v)\n",
    "                value = np.mean(np.asarray(value), axis=0)\n",
    "                plt.plot(steps, value, label=f'b = {b}, {method.__name__}')\n",
    "        plt.title(f'{n} states')\n",
    "\n",
    "        plt.ylabel('value of start state')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('computation time, in expected updates')\n",
    "\n",
    "    plt.savefig('./figure_8_8.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    figure_8_8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
