\documentclass{beamer}
\usetheme{metropolis} 
\usepackage{listings}
\usepackage{xcolor}
\definecolor{myBlue}{RGB}{0, 0, 180}
\definecolor{myGreen}{RGB}{34, 139, 34}
\definecolor{myRed}{RGB}{180, 0, 0}
\definecolor{myGray}{RGB}{96, 96, 96}
\lstset{ 
    language=Python,
    basicstyle=\ttfamily\tiny,
    keywordstyle=\color{myBlue},
    commentstyle=\color{myGreen},
    stringstyle=\color{myRed},
    showstringspaces=false,
    numberstyle=\tiny\color{myGray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    captionpos=b
}
\newcommand{\shrug}[1][]{%
\begin{tikzpicture}[baseline,x=0.8\ht\strutbox,y=0.8\ht\strutbox,line width=0.125ex,#1]
\def\arm{(-2.5,0.95) to (-2,0.95) (-1.9,1) to (-1.5,0) (-1.35,0) to (-0.8,0)};
\draw \arm;
\draw[xscale=-1] \arm;
\def\headpart{(0.6,0) arc[start angle=-40, end angle=40,x radius=0.6,y radius=0.8]};
\draw \headpart;
\draw[xscale=-1] \headpart;
\def\eye{(-0.075,0.15) .. controls (0.02,0) .. (0.075,-0.15)};
\draw[shift={(-0.3,0.8)}] \eye;
\draw[shift={(0,0.85)}] \eye;
% draw mouth
\draw (-0.1,0.2) to [out=15,in=-100] (0.4,0.95); 
\end{tikzpicture}}
\title{Artificial Intelligence, Algorithmic Pricing, and Collusion}
\subtitle{Emilio Calvano, Giacomo Calzolari, Vincenzo Denicol\`{o}, and Sergio Pastorello}
\date{\today}
\author{Gabe Sekeres and Finn Ye}
\institute{Cornell University}
\begin{document}


\begin{frame}
	\titlepage
\end{frame}


\section{Background}

\begin{frame}\frametitle{Motivation}
	\begin{itemize}
		\item Algorithms becoming increasingly prevalent in practice
		\begin{itemize}
			\item German gasoline markets (Assad et al. 2024)
			\item Smartphone price discrimination (Kehoe et al. 2020)
		\end{itemize}
		\item Regulatory questions: \begin{itemize} \item How do algorithms get to collusive prices? \item Can they do so in the absence of active principals? \item Is algorithmic collusion visibly different than tacit collusion? \end{itemize}
		\item Massive lack of theoretical guarantees for this (see Banchio and Mantegazza (2023); Possnig (2024); Lamba and Zhuk (2025))
	\end{itemize}	
\end{frame}

\begin{frame}\frametitle{Q-Learning Algorithms}
	\begin{itemize}
		\item We're familiar with Q-learners 
		\item Specifically, they learn \emph{slowly}. This example has a massive state space and is trying to learn the opponent's policy as part of the state
		\item Our biggest criticisms are related to this. Specifically: \begin{itemize} \item What is the loss in the learning phase? \item How sensitive are these results to the initialization of the Q-matrix?\end{itemize}
	\end{itemize}
\end{frame}

\section{Model}

\begin{frame}\frametitle{Environment}
	Canonical oligopoly pricing game, with $n$ firms / products and an outside good, where in each period $t$ the demand for good $i$ is \[q_{i,t} = \frac{e^{\frac{a_i-p_{i,t}}{\mu}}}{\sum_{j=1}^n e^{\frac{a_j - p_{j,t}}{\mu}} + e^{\frac{a_0}{\mu}}}\]where $a_i$ is an index of quality, $\mu$ is in index of differentiation, and $a_0$ is an outside good. Firms choose $p_{i,t}$, and we have exogenous marginal costs $c_i$. The stage problem is:\[\max_{p_{i,t}} q_{i,t}(p_{i,t}) \cdot p_{i,t} - q_{i,t}(p_{i,t}) \cdot c_{i,t}\]This is quasiconcave but does not in general have a nice closed form solution.
\end{frame}
\begin{frame}\frametitle{Simplified Stage Environment}
	Assume $n=2$, $c_i = 1$, $a_i = 2$, $a_0=0$, and $\mu = \frac{1}{4}$. Then the stage game reduces to\[\max_{p_i} \frac{(p_i - 1)e^{8 - 4p_{i}}}{e^{8 - 4p_{i}} + e^{8 - 4p_{j}} + 1}\]This is strictly concave, and we have that it admits Nash prices\[p^N_i = p^N_j \approx 1.473\]and monopoly prices are obtained from setting $n=1$, where we attain \[p^M = \frac{5}{4} - \frac{1}{4}W_n(2e^{3}) \approx 1.925\]
\end{frame}
\begin{frame}\frametitle{Simplified Stage Environment}
	We basically have an extension of a Prisoner's Dilemma:
	\[
	\begin{array}{c|cc}
		& N & M \\\hline N & (0.22,0.22) & (0.37,0.12) \\ M &(0.12,0.37) & (0.34,0.34)
	\end{array}	\]
	Since all of the involved functions are continuous and concave, this extends fairly nicely. 
	
	So we're making our Q-learners play a repeated Prisoner's Dilemma, and the strategies they learn \emph{should} be similar to canonical repeated PD strategies. 
\end{frame}

\begin{frame}\frametitle{Folk Theorem}
\begin{center}
	\begin{tikzpicture}[scale=1]
		\filldraw[blue,nearly transparent] (3.4,3.4)--(3.55,2.2)--(2.2,2.2)--(2.2,3.55)--(3.4,3.4);
		\draw[very thick, <->] (0,5)--(0,0)--(5,0);
		\draw[thick] (2.2,2.2)--(1.2,3.7)--(3.4,3.4)--(3.7,1.2)--(2.2,2.2);
		\filldraw (2.2,2.2) circle(2pt);
		\filldraw (1.2,3.7) circle(2pt);
		\filldraw (3.7,1.2) circle(2pt);
		\filldraw (3.4,3.4) circle(2pt);
		\draw[dashed,thick] (3.55,2.2)--(2.2,2.2)--(2.2,3.55);
		\node[below left] at (2.2,2.2) {\scriptsize$(0.22,0.22)$};
		\node[above right] at (3.4,3.4) {\scriptsize$(0.34,0.34)$};
		\node[right] at (3.7,1.2) {\scriptsize$(0.37,0.12)$};
		\node[above] at (1.2,3.7) {\scriptsize$(0.12,0.37)$};
	\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}\frametitle{Continuous Stage Payoffs}
	\includegraphics[width=10cm]{cont_plot.png}
\end{frame}

\begin{frame}\frametitle{A Question}
	Why use this sigmoid demand function instead of exogenously imposing a reasonable range for prices and using \emph{e.g.} linear demand?
	
	We don't understand what the gain from this functional form is, and the fact that it doesn't in general have closed-form solutions is an annoyance.
\end{frame}

\section{Learning Theory}

\begin{frame}\frametitle{Learning in Repeated Games}
	Essentially, take the opponent's previous actions to be the state, along with whatever game parameters you need. Two issues:
	\begin{enumerate}
		\item The state space is increasing as the game continues. \\ Solution: Bounded memory. 
		\item The optimization problem is non-stationary if the opponent(s) change strategy over time. \\ No official solutions here, this is why we don't have theoretical guarantees\footnote{I'm fairly sure there should be something here. At least in probability. I'm confused why nobody has proved that yet - Gabe}
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Learning in Repeated Games}
	The Q-learners solve \[Q(s,a) = \mathbb{E}(\pi \mid s,a) + \delta \mathbb{E}\left[ \max_{a'\in A} Q(s',a') \mid s,a\right]\] where $a \in A$ is the action (from the rules of the game), $s \in S$ is the state (defined as all player actions in the last $k$ periods, where $k$ is the memory). Once we discretize, $Q \in \mathbb{R}^{|A| \times |S|} = \mathbb{R}^{|A| \times |A|^{nk}}$
	
	For simplicity, $k=1$. Results robust to higher $k$.
\end{frame}

\begin{frame}\frametitle{Parameterization}
	Work in the simplified game as above, with $\delta = 0.95$ and $|A| = m = 15$. Discretize the price grid over \[\left[p^N - 0.1(p^M - p^N), p^M + 0.1(p^M - p^N)\right]\] Set the initial matrix to the discounted payoff if the other player randomized over all actions: \[Q_{i,0}(s,a_i) = \frac{\sum_{a_{-i} \in A^{n-1}}\pi_i(a_i,a_{-i})}{(1-\delta) |A|^{n-1}}\]Draw the initial state $s_0$ randomly as well.
\end{frame}

\begin{frame}\frametitle{A Difference}
	Their definition of $Q_0$ incorporates the fact that the game is infinite, and defines as if we are taking the discounted sum of payoffs forever, assuming that the other player uniformly randomizes.
	
	That's not how we've thought about this previously, because we're incorporating a meaningful discount factor for the first time. However, it means that $Q_0$ has a different qualitative meaning than we're used to. This is confusing and we don't particularly well understand the effects.
	
	(We test both, and show some results later)
\end{frame}
\begin{frame}\frametitle{Continuous Stage Payoffs}
\centering
	\includegraphics[width=10cm]{cont_plot.png}
\end{frame}
\begin{frame}\frametitle{Discretized Stage Payoffs}
\centering
	\includegraphics[width=8cm]{heatmap_plot.png}
\end{frame}

\begin{frame}\frametitle{Parameterization}
	We will use $\varepsilon$-greedy learners, with $\varepsilon_t = \exp(-\beta t)$. The learning parameter $\alpha$ will be tested over a grid of 100 points in $[0.025, 0.25]$, and several different values of $\beta$ are also tested. 
	
	They define $\nu$ to be the number of times a certain cell is visited in expectation under a certain $\beta$. For our purposes, $\beta$ will be tested over a grid of 100 points in $(0,2 \cdot 10^{-5})$. 
	
	Recall from earlier:\[Q(s,a) \leftarrow Q(s,a) + \alpha \left[ \pi(s',a') + \delta \max_{a'\in A} Q(s',a') - Q(s,a)\right]\]
\end{frame}

\begin{frame}\frametitle{Learning Visualization}
	\centering
	\begin{tikzpicture}[scale=2]
		\draw[thick] (0,0)--(2,0);
		\filldraw[color=black, fill=white] (0,0) circle(5pt);
		\node at (0,0) {\scriptsize$s$};
		\filldraw (1,0) circle(2pt);
		\filldraw[color=black, fill=white] (2,0) circle(5pt);
		\node at (2,0) {\scriptsize$s'$};
		\node[above right] at (1,0) {\scriptsize$\pi(a,a_{-1})$};
		\node[below left] at (1,0) {\scriptsize$a$};
		\draw[thick,->] (.2,-1.1) -- (.9,-1.1);
		\filldraw[color=black, fill=white] (0,-1.1) circle(7pt);
		\node at (0,-1.1) {\tiny$\left\langle p_1^{0},p_{2}^{0} \right\rangle$};
		
		\node[above] at (.5,-1.1) {\tiny$p_i^1$};
		\draw[thick,->] (1,-2) -- (1,-1.2);
		\filldraw (1,-1.1) circle(2pt);
		\node[right] at (1,-1.6) {\tiny$p_{-i}^1$};
		\node[above right] at (1,-1.1) {\tiny$\pi_i(p_1^1,p_2^1)$};
		\filldraw (0.9,-2)--(1.1,-2)--(1.1,-2.2)--(0.9,-2.2)--(0.9,-2);
		\draw[thick,->] (1.1,-1.1)--(1.74,-1.1);
		\filldraw[color=black, fill=white] (2,-1.1) circle(7pt);
		\node at (2,-1.1) {\tiny$\left\langle p_1^{1},p_{2}^{1}\right\rangle$};
	\end{tikzpicture}
\end{frame}

\begin{frame}\frametitle{Remark}
	Prior to this, we've generally dealt with RL algorithms that are trying to learn \alert{payoffs} in a static or stochastic game. These algorithms are (theoretically) learning \alert{strategies} for the infinitely repeated game. 
	
	Thinking about the bounded memory, that's no longer such an innocuous assumption. For example: these algorithms will be able to learn tit-for-tat or Grim Trigger, but not trigger strategies with $n$ periods of punishment for $n > k$.
\end{frame}


\section{Code}

\begin{frame}\frametitle{Python vs. Julia pt. $N$}
	\begin{itemize}
		\item The authors wrote the code in Fortran, which neither of us know because we were both born in 2001 
		\item Finn rewrote it in Python, and we'll present that code because it's the most similar to what we've seen previously
		\item Gabe refactored it into Julia and parallelized it, which increased the speed by a massive amount (2hrs $\to$ 5.5mins)
		\item The results below are from Julia, because it's more robust
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{High-level structure}
	\begin{itemize}
		\item Exact same as when we defined Q-learning previously, except that we now update two Q-matrices instead of one
		\item Rough pseudocode: 
		\begin{enumerate}
			\item Define $Q_0$ for each player, taking the expected value if other player chooses randomly
			\item Define action choice function (as always, $\varepsilon$-greedy)
			\item Iteratively learn, updating using the rule:\[Q(s,a) = \mathbb{E}(\pi \mid s,a) + \delta \mathbb{E}\left[ \max_{a'\in A} Q(s',a') \mid s,a\right]\]
			\item End when $100,000$ periods of convergence, defined as either \begin{itemize} \item[(i)] $s_t = (a^1_t,a^2_t) = (a^1_{t+1},a^2_{t+1}) = s_{t+1}$ \item[(ii)] $s_t = s_{t-2}$ \alert{and} $s_{t+1} = s_{t-1}$ \end{itemize}
			\item Return average per-firm profit once converged, compare to Nash / Monopoly profit
		\end{enumerate}
	\end{itemize}
	
\end{frame}

\begin{frame}\frametitle{Changes}
	We had to make some small changes to make the logic work. The main ones are:
	\begin{itemize}
		\item Added an arbitrary cutoff for saying convergence failed (10m)
		\item Allowed cycles of 2 to be defined as convergence
		\item Ran many fewer times (25 experiments, across a $15 \times 15$ grid)
	\end{itemize}
	These were mainly runtime issues. If we had access to a supercomputing cluster, we could do a significantly more robust replication (\emph{hint hint})
\end{frame}

\begin{frame}\frametitle{Parameterization}
	Exact same as in the paper, except we used 15 realizations of $\alpha$ and $\beta$, on the intervals $[0.1,0.2]$ and $[5 \cdot 10^{-6},1.5 \cdot 10^{-5}]$ respectively. Specifically, we have:
	\begin{center}
		\begin{tikzpicture}
			\node<1-> at (0,0) {\includegraphics[width=8cm]{calvano_fig_1.png}};
			\draw<2->[blue,thick] (-1.5,0) rectangle (1.2,1.6);
		\end{tikzpicture}
	\end{center}
	
\end{frame}


\section{Results}

\begin{frame}\frametitle{One Run}
	\begin{center}
		\includegraphics[width=9cm]{heatmap_profit_gain_once.png}
	\end{center}
\end{frame}


\begin{frame}\frametitle{25 Runs, no $(1-\delta)$}
	\begin{center}
		\includegraphics[width=9cm]{heatmap_profit_gain_25_nodelta.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{25 Runs, with $(1-\delta)$}
	\begin{center}
		\includegraphics[width=9cm]{heatmap_profit_gain_25_delta.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Convergence Rates, no $(1-\delta)$}
	\begin{center}
		\includegraphics[width=9cm]{heatmap_convergence_counts_25_nodelta.png}
	\end{center}
\end{frame}
\begin{frame}\frametitle{Convergence Rates, with $(1-\delta)$}
	\begin{center}
		\includegraphics[width=9cm]{heatmap_convergence_counts_25_delta.png}
	\end{center}
\end{frame}

\section{Results pt. 2}

\begin{frame}\frametitle{Specifications}
	For all: $25 \times 25$ grid of $(\alpha,\beta)$, on $\alpha \in [0.025,0.25]$ and $\beta \in (0,2\cdot 10^{-5}]$
	\begin{itemize}
		\item Baseline (with $\delta = 0.95$)
		\item Fix $\delta = 0$
		\item SARSA
		\item Full Feedback
		\item Sensitivity of $Q_0$ \begin{itemize} \item $Q_0 = 0$ \item $0.8 \cdot Q_0$ \item $0.9 \cdot Q_0$ \item \textcolor{red}{$1.1 \cdot Q_0$} \item \textcolor{red}{$1.2 \cdot Q_0$}\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Baseline Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_main_delta_0.95.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{Baseline Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_main_delta_0.95.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{Baseline Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_main_delta_0.95.png}
\end{center}
\end{frame}
	
	
	\begin{frame}\frametitle{$\delta=0$ Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_main_delta_0.0.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$\delta=0$ Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_main_delta_0.0.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$\delta=0$ Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_main_delta_0.0.png}
\end{center}
\end{frame}

	
	\begin{frame}\frametitle{SARSA Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_sarsa.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{SARSA Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_sarsa.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{SARSA Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_sarsa.png}
\end{center}
\end{frame}


	\begin{frame}\frametitle{Full Feedback Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_full_feedback.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{Full Feedback Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_full_feedback.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{Full Feedback Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_full_feedback.png}
\end{center}
\end{frame}

	\begin{frame}\frametitle{$Q_0=0$ Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_zero.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$Q_0=0$ Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_zero.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$Q_0=0$ Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_zero.png}
\end{center}
\end{frame}

	\begin{frame}\frametitle{$0.8 \cdot Q_0$ Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_0.8.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$0.8 \cdot Q_0$ Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_0.8.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$0.8 \cdot Q_0$ Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_0.8.png}
\end{center}
\end{frame}

	\begin{frame}\frametitle{$0.9 \cdot Q_0$ Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_0.9.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$0.9 \cdot Q_0$ Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_0.9.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$0.9 \cdot Q_0$ Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_0.9.png}
\end{center}
\end{frame}


	\begin{frame}\frametitle{$1.1 \cdot Q_0$ Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_1.1.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$1.1 \cdot Q_0$ Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_1.1.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$1.1 \cdot Q_0$ Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_1.1.png}
\end{center}
\end{frame}


	\begin{frame}\frametitle{$1.2 \cdot Q_0$ Profit Gain}
\begin{center}
	\includegraphics[width=9cm]{heatmap_profit_gain_small_1.2.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$1.2 \cdot Q_0$ Convergence}
\begin{center}
	\includegraphics[width=9cm]{heatmap_convergence_counts_small_1.2.png}
\end{center}
\end{frame}
\begin{frame}\frametitle{$1.2 \cdot Q_0$ Actual Prices}
\begin{center}
	\includegraphics[width=9cm]{scatter_plot_prices_small_1.2.png}
\end{center}
\end{frame}




	
\end{document}
